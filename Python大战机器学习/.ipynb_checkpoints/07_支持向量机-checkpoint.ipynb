{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T08:03:15.677831Z",
     "start_time": "2018-12-27T08:03:15.670830Z"
    }
   },
   "source": [
    "### 线性可分支持向量机\n",
    "给定线性可分训练数据集$T$，假设通过间隔最大化学习得到的分离超平面为：$\\vec{w}^* \\cdot \\vec{x}+b^*=0$，定义分类决策函数：$f(\\vec{x})=sign(\\vec{w}^* \\cdot \\vec{x}+b^*)$，该分类决策函数也称为线性可分支持向量机。\n",
    "\n",
    "给定超平面$\\vec{w} \\cdot \\vec{x}+b=0$，样本$\\vec{x}_i$距超平面的距离为：$|\\vec{w} \\cdot \\vec{x}_i+b|$。$\\vec{w} \\cdot \\vec{x}_i+b$的符号与样本标记$y_i$的符号是否一致表示分类是否正确。\n",
    "\n",
    "函数间隔的缺陷：当成比例地改变$\\vec{w}$和$b$，超平面不变，但是函数间隔也会成比例的变化。\n",
    "\n",
    "对于给定的训练数据集$T$和超平面$(\\vec{w},b)$，定义超平面关于样本点$(\\vec{x}_i,y_i)$的几何间隔为：$$\\gamma_i=y_i(\\frac{\\vec{w}}{||\\vec{w}||}\\cdot \\vec{x}_i+\\frac{b}{||\\vec{w}||})$$\n",
    "\n",
    "- 支持向量中的正例位于超平面$H_1$：$\\vec{w}^* \\cdot \\vec{x}+b^*=1$\n",
    "- 支持向量中的负例位于超平面$H_2$：$\\vec{w}^* \\cdot \\vec{x}+b^*=-1$\n",
    "\n",
    "____\n",
    "线性可分支持向量机的原始最优化问题：\n",
    "$$\\begin{matrix}\n",
    "\\underset{\\vec{w},b}{min}\\frac{1}{2}||\\vec{w}||_2^2 \\\\ \n",
    "s.t.\\ \\ y_i(\\vec{w}\\cdot \\vec{x}_i+b)-1 \\geq 0, i=1,2,\\cdots,N\n",
    "\\end{matrix}$$\n",
    "求得最优解$\\vec{w}^*,b^*$\n",
    "\n",
    "_____\n",
    "线性可分支持向量机对偶算法：\n",
    "$$\\begin{matrix}\n",
    "\\underset{\\vec{\\alpha}}{min}\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N\\alpha_i \\alpha_j y_iy_j(\\vec{x}_i\\cdot \\vec{x}_j)-\\sum_{i=1}^N\\alpha_i\\\\ \n",
    "s.t.\\ \\sum_{i=1}^N\\alpha_iy_i=0\\\\ \n",
    "\\alpha_i \\geq 0, i=1,2,\\cdots,N\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性支持向量机\n",
    "线性支持向量机的原始问题：\n",
    "$$\\begin{matrix}\n",
    "\\underset{\\vec{w},b,\\xi}{min}\\{\\frac{1}{2}||\\vec{w}||_2^2 +C\\sum_{i=1}^N\\xi_i\\}\\\\ \n",
    "s.t.\\ \\ y_i(\\vec{w}\\cdot \\vec{x}_i+b) \\geq 1-\\xi_i, i=1,2,\\cdots,N\\\\\n",
    "\\xi_i\\geq 0,i=1,2,\\cdots,N\n",
    "\\end{matrix}$$\n",
    "\n",
    "____\n",
    "线性支持向量机的对偶算法：\n",
    "$$\\begin{matrix}\n",
    "\\underset{\\vec{\\alpha}}{min}\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N\\alpha_i \\alpha_j y_iy_j(\\vec{x}_i\\cdot \\vec{x}_j)-\\sum_{i=1}^N\\alpha_i\\\\ \n",
    "s.t.\\ \\sum_{i=1}^N\\alpha_iy_i=0\\\\ \n",
    "0\\leq \\alpha_i \\leq C, i=1,2,\\cdots,N\n",
    "\\end{matrix}$$\n",
    "此时的分类决策函数为：$f(\\vec{x})=sign(\\vec{w}^*\\cdot \\vec{x}+b^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "- 多项式核函数：$K(\\vec{x},\\vec{z})=(\\vec{x}\\cdot \\vec{z}+1)^p$\n",
    "- 高斯核函数：$K(\\vec{x},\\vec{z})=exp(-\\gamma||x-z||_2^2)$\n",
    "- sigmoid核函数：$K(\\vec{x},\\vec{z})=tanh(\\gamma(\\vec{x}\\cdot \\vec{z})+r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量回归（Support Vector Regression, SVR）\n",
    "- 允许$f(\\vec{x}_i)$与$y_i$之间最多有$\\epsilon$的偏差。仅当$|f(\\vec{x}_i)-y_i|>\\epsilon$时，才计算损失。当$|f(\\vec{x}_i)-y_i|\\leq \\epsilon$时，认为预测正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "- SVM对缺失数据敏感，对非线性问题没有通用解决方案，必须谨慎选择核函数来处理，计算复杂度高。\n",
    "- 其存在两个对结果影响相当大的超参数（若用RBF核，是核函数的gamma参数以及惩罚项C），这两个超参数无法通过概率方法进行计算，只能通过穷举试验来求出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T01:45:08.007967Z",
     "start_time": "2018-12-28T01:45:06.748895Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model, svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T01:47:07.558805Z",
     "start_time": "2018-12-28T01:47:07.552805Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_regression():\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    return train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T01:51:02.782259Z",
     "start_time": "2018-12-28T01:51:02.778259Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_classification():\n",
    "    iris = datasets.load_iris()\n",
    "    X_train = iris.data\n",
    "    y_train = iris.target\n",
    "    return train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性分类LinearSVC\n",
    "**sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)**\n",
    "> 1. penalty：字符串。惩罚的范数，指定l1或l2。\n",
    "> 2. loss：字符串，表示损失函数。\n",
    "    - hinge：合页损失函数\n",
    "    - squared_hinge：合页损失函数的平方\n",
    "> 3. dual：布尔值。若为True，则解决对偶问题；若为False，则解决原始问题。当n_samples > n_features时，倾向于采用False。\n",
    "> 4. C：浮点数，罚项系数。\n",
    "    - 衡量了误分类点的重要性，C越大则误分类点越重要。\n",
    "> 5. multi_class：字符串，指定多类分类问题的策略。\n",
    "    - ovr：采用one-vs-rest策略\n",
    "    - crammer-singer：多类联合分类。计算量大。\n",
    "> 6. fit_intercept：布尔值，若为True，则计算截距，即决策函数中的常数项。\n",
    "> 7. intercept_scaling：浮点值。如果提供了，实例$X$变成向量[X, intercept_scaling]，相当于添加了一个人工特征。\n",
    "> 8. class_weight：字典、或字符串balanced。指定各个类的权重，若未提供，则认为类的权重为1。\n",
    "    - balanced：每个类的权重为它出现频率的倒数\n",
    "\n",
    "属性：\n",
    "> 1. coef_：数组，给出了各个特征的权重。\n",
    "> 2. intercept_：数组，给出了截距，即决策函数中的常数项。\n",
    "\n",
    "方法：\n",
    "> 1. fit(X, y)\n",
    "> 2. predict(X)\n",
    "> 3. score(X, y[,sample_weight])：返回在(X,y)上预测的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:12:44.652722Z",
     "start_time": "2018-12-28T02:12:44.631721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofficients:[[ 0.20959412  0.3992384  -0.81739092 -0.44232033]\n",
      " [-0.12194514 -0.78738859  0.52434181 -1.05548013]\n",
      " [-0.80301283 -0.87621252  1.21361223  1.81023049]], intercept [ 0.11973659  2.0396631  -1.44393678]\n",
      "Score: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python36\\p36env\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def test_LinearSVC(*data):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    cls = svm.LinearSVC()\n",
    "    cls.fit(X_train, y_train)\n",
    "    print(\"Cofficients:%s, intercept %s\"% (cls.coef_, cls.intercept_))\n",
    "    print(\"Score: %.2f\" % cls.score(X_test, y_test))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_data_classification()\n",
    "test_LinearSVC(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非线性分类SVC\n",
    "**sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)**\n",
    "> 1. kernel：字符串，指定核函数。\n",
    "    - linear\n",
    "    - poly\n",
    "    - rbf\n",
    "    - sigmoid\n",
    "> 2. degree：整数。当核函数是多项式核函数时，多项式的系数。\n",
    "> 3. gamma：浮点数。当核函数是rbf、poly、sigmoid时，核函数的系数；若为auto，表示系数为1/n_features。\n",
    "> 4. coef0：浮点数。指定核函数中的自由项，适用于poly、sigmoid核。\n",
    "> 5. decision_function_shape：字符串或None，指定决策函数的形状。\n",
    "    - ovr：使用one-vs-rest准则，决策函数形状是(n_samples, n_classes)。对每个分类定义了一个二类SVM，一共n_classes个二类SVM组合成一个多类SVM。\n",
    "    - ovo：使用one-vs-one准则，决策函数形状是(n_samples, n_classes * (n_classes - 1)/2)。对每一对分类直接定义了一个二类SVM。\n",
    "    \n",
    "属性：\n",
    "> 1. support_：数组，形状为[n_SV]，支持向量的下标。\n",
    "> 2. support_vectors_：数组，形状为[n_SV, n_features]，支持向量\n",
    "> 3. n_support_：数组，形状为[n_class]，每一个分类的支持向量的个数。\n",
    "> 4. dual_coef_：数组，形状为[n_class-1, n_SV]。对偶问题中，在分类决策函数中每个支持向量的系数。\n",
    "> 5. coef_：数组，形状为[n_class-1, n_features]。原始问题中，每个特征的系数。只在linear核中有效。\n",
    "> 6. intercept_：数组，形状为[n_class * (n_class - 1) / 2]，决策函数中的常数项。\n",
    "\n",
    "方法：\n",
    "> 1. fit(X, y[, sample_weight])\n",
    "> 2. predict(X)\n",
    "> 3. score(X, y[,sample_weight])：返回在(X,y)上预测的准确率。\n",
    "> 4. predict_log_proba(X)：返回数组，元素依次为X预测为各个类别的概率的对数值。\n",
    "> 5. predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:39:29.501514Z",
     "start_time": "2018-12-28T02:39:29.491514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofficients:[[-0.16990304  0.47442881 -0.93075307 -0.51249447]\n",
      " [ 0.02439178  0.21890135 -0.52833486 -0.25913786]\n",
      " [ 0.52289771  0.95783924 -1.82516872 -2.00292778]], intercept [2.0368826 1.1512924 6.3276538]\n",
      "Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "def test_SVC_linear(*data):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    cls = svm.SVC(kernel='linear')\n",
    "    cls.fit(X_train, y_train)\n",
    "    print(\"Cofficients:%s, intercept %s\"% (cls.coef_, cls.intercept_))\n",
    "    print(\"Score: %.2f\" % cls.score(X_test, y_test))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_data_classification()\n",
    "test_SVC_linear(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归LinearSVR\n",
    "**sklearn.svm.LinearSVR(epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)**\n",
    "> 1. epsilon：浮点数，用于lose中的$\\epsilon$参数\n",
    "> 2. loss：字符串，表示损失函数。\n",
    "    - epsilon_insensitive：此时损失函数为$L_{\\epsilon}$\n",
    "    - squared_epsilon_insensitive：损失函数为$L_{\\epsilon}^2$\n",
    "\n",
    "属性：\n",
    "> 1. coef_：数组，给出各个特征的权重。\n",
    "> 2. intercept_：数组，给出了截距，即决策函数中的常数项。\n",
    "\n",
    "方法：\n",
    "> 1. fit(X, y)\n",
    "> 2. predict(X)\n",
    "> 3. score(X, y[,sample_weight])：返回在(X,y)上预测的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:51:18.346058Z",
     "start_time": "2018-12-28T02:51:18.275054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofficients:[ 2.14940259  0.4418875   6.35258779  4.62357282  2.82085901  2.42005063\n",
      " -5.3367464   5.41765142  7.26812843  4.33778867], intercept [99.]\n",
      "Score: -0.56\n"
     ]
    }
   ],
   "source": [
    "def test_LinearSVR(*data):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    regr = svm.LinearSVR()\n",
    "    regr.fit(X_train, y_train)\n",
    "    print(\"Cofficients:%s, intercept %s\"% (regr.coef_, regr.intercept_))\n",
    "    print(\"Score: %.2f\" % regr.score(X_test, y_test))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_data_regression()\n",
    "test_LinearSVR(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:53:20.486044Z",
     "start_time": "2018-12-28T02:53:20.477043Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-02, 1.20679264e-02, 1.45634848e-02, 1.75751062e-02,\n",
       "       2.12095089e-02, 2.55954792e-02, 3.08884360e-02, 3.72759372e-02,\n",
       "       4.49843267e-02, 5.42867544e-02, 6.55128557e-02, 7.90604321e-02,\n",
       "       9.54095476e-02, 1.15139540e-01, 1.38949549e-01, 1.67683294e-01,\n",
       "       2.02358965e-01, 2.44205309e-01, 2.94705170e-01, 3.55648031e-01,\n",
       "       4.29193426e-01, 5.17947468e-01, 6.25055193e-01, 7.54312006e-01,\n",
       "       9.10298178e-01, 1.09854114e+00, 1.32571137e+00, 1.59985872e+00,\n",
       "       1.93069773e+00, 2.32995181e+00, 2.81176870e+00, 3.39322177e+00,\n",
       "       4.09491506e+00, 4.94171336e+00, 5.96362332e+00, 7.19685673e+00,\n",
       "       8.68511374e+00, 1.04811313e+01, 1.26485522e+01, 1.52641797e+01,\n",
       "       1.84206997e+01, 2.22299648e+01, 2.68269580e+01, 3.23745754e+01,\n",
       "       3.90693994e+01, 4.71486636e+01, 5.68986603e+01, 6.86648845e+01,\n",
       "       8.28642773e+01, 1.00000000e+02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:53:31.498674Z",
     "start_time": "2018-12-28T02:53:31.490673Z"
    }
   },
   "source": [
    "#### 非线性回归SVR\n",
    "**sklearn.svm.SVR(kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)**\n",
    "\n",
    "属性：\n",
    "> 1. support_：数组，形状为[n_SV]，支持向量的下标。\n",
    "> 2. support_vectors_：数组，形状为[n_SV, n_features]，支持向量\n",
    "> 3. n_support_：数组，形状为[n_class]，每一个分类的支持向量的个数。\n",
    "> 4. dual_coef_：数组，形状为[n_class-1, n_SV]。对偶问题中，在分类决策函数中每个支持向量的系数。\n",
    "> 5. coef_：数组，形状为[n_class-1, n_features]。原始问题中，每个特征的系数。只在linear核中有效。\n",
    "> 6. intercept_：数组，形状为[n_class * (n_class - 1) / 2]，决策函数中的常数项。\n",
    "\n",
    "方法：\n",
    "> 1. fit(X, y[, sample_weight])\n",
    "> 2. predict(X)\n",
    "> 3. score(X, y[,sample_weight])：返回在(X,y)上预测的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T02:58:17.007004Z",
     "start_time": "2018-12-28T02:58:16.972002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofficients:[[ 2.24127622 -0.38128702  7.87018376  5.21135861  2.26619436  1.70869458\n",
      "  -5.7746489   5.51487251  7.94860817  4.59359657]], intercept [137.11012796]\n",
      "Score: -0.03\n"
     ]
    }
   ],
   "source": [
    "def test_SVR_linear(*data):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    regr = svm.SVR(kernel='linear')\n",
    "    regr.fit(X_train, y_train)\n",
    "    print(\"Cofficients:%s, intercept %s\"% (regr.coef_, regr.intercept_))\n",
    "    print(\"Score: %.2f\" % regr.score(X_test, y_test))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_data_regression()\n",
    "test_SVR_linear(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
