{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑斯蒂分布\n",
    "- 设$X$是连续随机变量，$X$服从逻辑斯蒂分布是指$X$具有下列分布函数和密度函数：\n",
    "$$F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$$\n",
    "$$f(x)=F'(x)=\\frac{e^{-(x-\\mu)\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$$\n",
    "其中，$\\mu$为位置参数，$\\gamma >0$为形状参数，$\\gamma$的值越小，曲线在中心附近增长得越快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "#### 逻辑斯蒂回归模型\n",
    "- 二项逻辑斯蒂回归模型是如下的条件概率分布：\n",
    "$$P(Y=1|x)=\\frac{exp(w\\cdot x + b)}{1+exp(w\\cdot x + b)}$$\n",
    "$$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x + b)}$$\n",
    "比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "- 一个事件的机率(odds)：指该事件发生的概率与该事件不发生的概率的比值。对数几率函数是：$$log\\frac{p}{1-p}$$\n",
    "\n",
    "对逻辑斯蒂回归而言，得\n",
    "$$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x + b$$\n",
    "即，输出$Y=1$的对数几率是输入$x$的线性函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "#### 多项逻辑斯蒂回归\n",
    "假设离散随机变量$Y$的取值集合是$\\{1,2,\\cdots,K\\}$，那么多项逻辑斯蒂回归模型是：\n",
    "$$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}, k=1,2,\\cdots, K-1$$\n",
    "$$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "#### 最大熵模型\n",
    "- 学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。\n",
    "\n",
    "- 最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件，在没有更多信息的情况下，那些不确定的部分都是“等可能的”。\n",
    "\n",
    "最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。\n",
    "\n",
    "最大熵模型的一般形式为：\n",
    "$$P_w(y|x)=\\frac{1}{Z_w(x)}exp(\\sum_{i=1}^nw_if_i(x,y))$$\n",
    "其中，$$Z_w(x)=\\sum_yexp(\\sum_{i=1}^nw_if_i(x,y))$$\n",
    "对数似然函数为：\n",
    "$$L(w)=\\sum_{x,y}\\tilde{P}(x,y)\\sum_{i=1}^nw_if_i(x,y)-\\sum_x\\tilde{P}(x)logZ_w(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 优化算法\n",
    "1. 迭代尺度法\n",
    "2. 改进的迭代尺度法(IIS)\n",
    "    - 假设最大熵模型当前的参数向量是$w=(w_1,w_2,\\cdots,w_n)^T$，希望找到一个新的参数向量$w+\\sigma=(w_1+\\sigma_1,w_2+\\sigma_2,\\cdots, w_n+\\sigma_n)^T$，使得模型的对数似然函数值增大。如果能有这样一种参数向量更新的方法$\\tau:\\ w\\rightarrow w+\\sigma$，那么就可以重复使用这一方法，直至找到对数似然函数的最大值。\n",
    "\n",
    "3. 梯度下降法\n",
    "4. 牛顿法\n",
    "5. 拟牛顿法：DFP、BFGS、L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑如下无约束的极小化问题：$$\\underset{X}{min}f(X))$$\n",
    "其中，$X=(x_1,x_2,\\cdots,x_N)^T$，假定$f$为凸函数，且两阶连续可微。\n",
    "\n",
    "##### 牛顿法\n",
    "考虑$N=1$的简单情形，此时目标函数变为$f(x)$。\n",
    "\n",
    "牛顿法的基本思想为：在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值。设$x_k$为当前的极小点估计值，则：\n",
    "$$\\phi(x)=f(x_k)+f'(x_k)(x-x_k)+\\frac{1}{2}f''(x_k)(x-x_k)^2$$\n",
    "\n",
    "由于求的是最值，由极值必要条件可知，$\\phi(x)$应满足：$$\\phi'(x)=0$$\n",
    "即：$$f'(x_k)+f''(x_k)(x-x_k)=0$$\n",
    "得到：$$x=x_k-\\frac{f'(x_k)}{f''(x_k)}$$\n",
    "若给定初始值$x_0$，则可构造如下的迭代格式：$$x_{k+1}=x_k-\\frac{f'(x_k)}{f''(x_k)},\\ k=0,1,\\cdots$$\n",
    "_____\n",
    "\n",
    "对于$N>1$的情形，二阶泰勒展开式为：\n",
    "$$\\phi(x)=f(X_k)+\\triangledown f(X_k)\\cdot (X-X_k)+\\frac{1}{2}(X-X_k)^T\\cdot \\triangledown ^2f(X_k)\\cdot (X-X_k)$$\n",
    "其中，$\\triangledown f$为$f$的梯度向量，简记为$g_k$。$\\triangledown ^2f$为$f$的海森矩阵，简记为$H_k$。\n",
    "\n",
    "给定初值$X_0$，可构造迭代格式为：$$X_{k+1}=X_k-H_k^{-1}\\cdot g_k,\\ k=0,1,\\cdots$$\n",
    "_____\n",
    "完整的算法：\n",
    "1. 给定初值$X_0$和精度阈值$\\epsilon$，并令$k:=0$；\n",
    "2. 计算$g_k$和$H_k$；\n",
    "3. 若$||g_k||<\\epsilon$，则停止迭代；否则确定搜索方向$d_k=-H_k^{-1}\\cdot g_k$；\n",
    "4. 计算新的迭代点：$X_{k+1}=X_k+d_k$；\n",
    "5. 令$k:=k+1$，转至步2。\n",
    "____\n",
    "优点：\n",
    "1. 不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，考虑了梯度变化的趋势，能更全面地确定合适的搜索方向以加快收敛。\n",
    "\n",
    "缺点：\n",
    "1. 对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定；\n",
    "2. 计算复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 拟牛顿法\n",
    "- 不用二阶偏导数而构造出可以近似海森矩阵的正定对称阵。\n",
    "\n",
    "记$B$为海森矩阵$H$的近似，$D$为海森矩阵的逆$H^{-1}$的近似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考链接\n",
    "- https://www.cnblogs.com/ljy2013/p/5129294.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
