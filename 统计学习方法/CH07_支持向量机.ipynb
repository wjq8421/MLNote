{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 支持向量机的学习算法是求解凸二次规划的最优化算法。\n",
    "\n",
    "核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持向量机，等价于隐式地在高位的特征空间中学习线性支持向量机。这样的方法称为核技巧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性可分支持向量机\n",
    "- 给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为$$w^*\\cdot x + b^*=0$$\n",
    "以及相应的分类决策函数$$f(x)=sign(w^*\\cdot x + b^*)$$\n",
    "称为线性可分支持向量机。\n",
    "\n",
    "$|w\\cdot x + b|$能够相对地表示点$x$距离超平面的远近，而$w\\cdot x + b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可用量$y(w\\cdot x + b)$来表示分类的正确性及确信度。\n",
    "\n",
    "- 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的**函数间隔**为：$$\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$$\n",
    "定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即$$\\hat{\\gamma}=\\underset{i=1,\\cdots,N}{min}\\hat{\\gamma}_i$$\n",
    "\n",
    "对分离超平面的法向量$w$加某些约束，如规范化，$||w||=1$，使得间隔是确定的。\n",
    "\n",
    "- 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b))$关于样本点$(x_i,y_i)$的**几何间隔**为：$$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i + \\frac{b}{||w||})$$\n",
    "定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即\n",
    "$$\\gamma = \\underset{i=1,\\cdots,N}{min}\\gamma _i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大间隔法（maximum margin method）\n",
    "> 支持向量机学习的基本思想：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。\n",
    "\n",
    "最大间隔分离超平面的约束最优化问题为：\n",
    "$$\\begin{matrix}\n",
    "\\underset{w,b}{max}\\ \\gamma\\\\ \n",
    "s.t.\\ y_i(\\frac{w}{||w||}\\cdot x_i + \\frac{b}{||w||})\\geq \\gamma,\\ i=1,2,\\cdots, N\n",
    "\\end{matrix}$$\n",
    "\n",
    "考虑几何间隔与函数间隔的关系，上述公式可改写成：\n",
    "$$\\begin{matrix}\n",
    "\\underset{w,b}{max}\\ \\frac{\\hat{\\gamma}}{||w||}\\\\ \n",
    "s.t.\\ y_i(w\\cdot x_i + b)\\geq \\hat{\\gamma},\\ i=1,2,\\cdots, N\n",
    "\\end{matrix}$$\n",
    "\n",
    "由于函数间隔$\\hat{\\gamma}$的取值并不影响最优化问题的解，假设$w$和$b$按比例变为$\\lambda w$和$\\lambda b$，此时函数间隔成为$\\lambda \\hat{\\gamma}$。这一改变对上面最优化问题的不等式约束没有影响。因此，取$\\hat{\\gamma}=1$，线性可分支持向量机的最优化问题为：\n",
    "$$\\begin{matrix}\n",
    "\\underset{w,b}{min}\\ \\frac{1}{2}||w||^2\\\\ \n",
    "s.t.\\ y_i(w\\cdot x_i + b)-1\\geq 0,\\ i=1,2,\\cdots, N\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "- 支持向量（support vector）：在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例。支持向量是使约束条件等号成立的点，即\n",
    "$$y_i(w\\cdot x_i +b) - 1 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对偶算法\n",
    "优点：1）对偶问题往往更容易求解；2）自然引入核函数，进而推广到非线性分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "1. 首先引进拉格朗日乘子$\\alpha_i \\geq 0,\\ i=1,2,\\cdots,N$，定义拉格朗日函数为：\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N\\alpha_iy_i(w\\cdot x_i+b)+\\sum_{i=1}^N\\alpha_i$$\n",
    "原始问题的对偶问题是极大极小问题：\n",
    "$$\\underset{\\alpha}{max}\\ \\underset{w,b}{min}L(w,b,\\alpha)$$\n",
    "\n",
    "2. 求$\\underset{w,b}{min}L(w,b,\\alpha)$\n",
    "\n",
    "将拉格朗日函数$L(w,b,\\alpha)$分别对$w,b$求偏导数，并令其等于0。得：\n",
    "$$w=\\sum_{i=1}^N\\alpha_iy_ix_i$$\n",
    "$$\\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "将上式代回拉格朗日函数，得：\n",
    "$$\\underset{w,b}{min}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+\\sum_{i=1}^N\\alpha_i$$\n",
    "\n",
    "3. 求$\\underset{w,b}{min}L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题，并将目标函数由极大转换成求极小，得：\n",
    "$$\\underset{\\alpha}{min}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$$\n",
    "$$s.t.\\ \\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "$$\\alpha_i\\geq 0,\\ i=1,2,\\cdots,N$$\n",
    "\n",
    "设$\\alpha^*=(\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_l^*)^T$是上述对偶问题的解，则存在下标$j$，使得$\\alpha_j^*>0$，并可按下式求得原始最优化问题的解$w^*,b^*$：\n",
    "$$w^*=\\sum_{i=1}^N\\alpha_i^*y_ix_i$$\n",
    "$$b^*=y_j-\\sum_{i=1}^N\\alpha_i^*y_i(x_i\\cdot x_j)$$\n",
    "\n",
    "4. 由此得，分离超平面为：\n",
    "$$\\sum_{i=1}^N\\alpha_i^*y_i(x\\cdot x_i)+b^*=0$$\n",
    "分类决策函数，即线性可分支持向量机的对偶形式可写成：\n",
    "$$f(x)=sign(\\sum_{i=1}^N\\alpha_i^*y_i(x\\cdot x_i)+b^*)$$\n",
    "\n",
    "在线性可分支持向量机中，$w^*$和$b^*$只依赖于训练数据中对应于$\\alpha_i^*>0$的样本点$(x_i,y_i)$，因此，将训练数据中对应于$\\alpha_i^*>0$的实例点称为支持向量，有\n",
    "$$y_i(w^*\\cdot x_i+b^*)-1=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T07:04:59.675331Z",
     "start_time": "2019-05-15T07:04:59.665330Z"
    }
   },
   "source": [
    "### 线性支持向量机\n",
    "> 线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔小于等于$1$的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\\xi_i\\geq 0$，使函数间隔加上松弛变量大于等于$1$。此时，约束条件变为：\n",
    "$$y_i(w\\cdot x_i + b)\\geq 1-\\xi_i$$\n",
    "\n",
    "同时，对每个松弛变量$\\xi_i$，支付一个代价$\\xi_i$，此时，线性支持向量机的学习问题变成如下凸二次规划问题：\n",
    "$$\\underset{w,b,\\xi}{min}\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$$\n",
    "$$s.t.\\ y_i(w\\cdot x_i+b)\\geq 1-\\xi_i,\\ i=1,2,\\cdots,N$$\n",
    "$$\\xi_i\\geq 0,\\ i=1,2,\\cdots,N$$\n",
    "其中，$C>0$称为惩罚参数，$C$值大时对误分类的惩罚增大。\n",
    "\n",
    "软间隔的支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。\n",
    "1. 若$\\alpha_i^*<C$，$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；\n",
    "2. 若$\\alpha_i^*=C$，$0<\\xi_i<1$，则分类正确，$x_i$在间隔边界与分离超平面之间；\n",
    "3. 若$\\alpha_i^*=C$，$\\xi_i=1$，则$x_i$在分离超平面上；\n",
    "4. 若$\\alpha_i^*=C$，$\\xi_i>1$，则$x_i$位于分离超平面误分一侧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非线性支持向量机\n",
    "> 用线性分类方法求解非线性分类问题的步骤：1）首先使用一个变换将原空间的数据映射到新空间；2）然后在新空间里用线性分类学习方法从训练数据中学习分类模型。\n",
    "\n",
    "在对偶问题的目标函数中的内积$x_i\\cdot x_j$可以用核函数$K(x_i,x_j)=\\phi(x_i)\\cdot \\phi(x_j)$来代替。此时对偶问题的目标函数变为：\n",
    "$$W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i$$\n",
    "分类决策函数变为：\n",
    "$$f(x)=sign(\\sum_{i=1}^N\\alpha_i^*y_iK(x_i,x)+b^*)$$\n",
    "等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间。\n",
    "\n",
    "在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间中进行的，不需要显式地定义特征空间和映射函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### 常用核函数\n",
    "1. 多项式核函数（polynomial kernel function）$$K(x,z)=(x\\cdot z +1)^p$$\n",
    "2. 高斯核函数（Gaussian kernel function）$$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$$\n",
    "对应的支持向量机是高斯径向基函数（radial basis function）分类器\n",
    "3. 字符串核函数（string kernel function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:11:47.282750Z",
     "start_time": "2019-05-16T01:11:47.265749Z"
    }
   },
   "source": [
    "______\n",
    "### 序列最小最优化算法（SMO，sequential minimal optimization）\n",
    "求解如下的凸二次规划的对偶问题：\n",
    "$$\\underset{\\alpha}{min}\\ \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i$$\n",
    "$$s.t.\\ \\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "$$0\\leq \\alpha_i \\leq C,\\ i=1,2,\\cdots,N$$\n",
    "\n",
    "> 基本思路：如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解。如此，SMO算法将原问题不断分解为子问题求解，进而达到求解原问题的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T06:32:46.987985Z",
     "start_time": "2019-07-18T06:32:46.882979Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
