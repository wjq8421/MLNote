{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择\n",
    "\n",
    "##### 信息增益\n",
    "- 直观上按照某一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。\n",
    "\n",
    "_____\n",
    "设$X$是一个取有限个值的离散随机变量，其概率分布为：\n",
    "$$P(X=x_i)=p_i,\\ i=1,2,\\cdots n$$\n",
    "则，随机变量$X$的**熵**为：\n",
    "$$H(X)=-\\sum_{i=1}^np_ilogp_i$$\n",
    "熵越大，随机变量的不确定性就越大。\n",
    "______\n",
    "随机变量$X$给定的条件下随机变量$Y$的**条件熵**$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：\n",
    "$$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)$$\n",
    "______\n",
    "- **信息增益**：表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。\n",
    "> 特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下的经验条件熵$H(D|A)$之差，即\n",
    "> $$g(D,A)=H(D)-H(D|A)$$\n",
    "> $H(D)$：表示对数据集$D$进行分类的不确定性。$H(D|A)$：表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性。\n",
    "\n",
    "设有$K$个类$C_k$，特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$\n",
    "$$H(D)=\\sum_{k=1}^K\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$\n",
    "$$H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{D_{ik}}{D_i}log_2\\frac{|D_{ik}|}{|D_i|}$$\n",
    "\n",
    "- 特征选择方法：对训练数据集$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。\n",
    "- 缺点：在训练数据集的经验熵大的时候，信息增益值会偏大；反之，信息增益值会偏小。\n",
    "\n",
    "##### 信息增益比\n",
    "$$g_R(D,A)=\\frac{g(D,A)}{H(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树的生成\n",
    "\n",
    "##### ID3算法\n",
    "- 核心：在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。\n",
    "- 缺点：只有树的生成，其生成的树容易产生过拟合。\n",
    "\n",
    "##### C4.5算法\n",
    "- 用信息增益比来选择特征。\n",
    "\n",
    "#### 决策树的剪枝(pruning)\n",
    "- 上述产生的树，往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。解决的方法：考虑决策树的复杂度，对已生成的决策树进行简化。\n",
    "\n",
    "决策树的剪枝通过极小化决策树整体的损失函数来实现。设树的叶结点个数为$T$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\\alpha\\geq 0$为参数，损失函数定义为：\n",
    "$$C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha |T|$$\n",
    "其中，经验熵为：\n",
    "$$H_t(T)=\\sum_k\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$$\n",
    "记，$$C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)$$\n",
    "$C(T)$表示模型对训练数据的预测误差，$|T|$表示模型复杂度，参数$\\alpha \\geq$控制两者之间的影响。剪枝，就是当$\\alpha$确定时，选择损失函数最小的模型。\n",
    "\n",
    "- 决策树生成学习局部的模型，而决策树剪枝学习整体的模型。\n",
    "_______\n",
    "设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$和$T_A$，其对应的损失函数值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，若$$C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$$\n",
    "则进行剪枝，即将父结点变为新的叶结点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART\n",
    "- 在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”。\n",
    "\n",
    "对回归树用**平方误差最小化**准则，对分类树用**基尼指数**最小化准则，进行特征选择，生成二叉树。\n",
    "\n",
    "#####  回归树的生成\n",
    "假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots, R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：$$f(x)=\\sum_{m=1}^Mc_mI(x\\in R_m)$$\n",
    "用平方误差最小的准则求解每个单元上的最优输出值，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上的所有实例$x_i$对应的输出$y_i$的均值，即$$\\hat{c}_m=ave(y_i|x_i\\in R_m)$$\n",
    "\n",
    "_____\n",
    "- Q：怎样对输入空间进行划分？\n",
    "\n",
    "选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：\n",
    "$$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$$\n",
    "和$$R_2(j,s)=\\{x|x^{(j)}\\geq s\\}$$\n",
    "然后寻找最优切分变量$j$和最优切分点$s$，即求解：\n",
    "$$\\underset{j,s}{min}[\\underset{c_1}{min}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\underset{c_2}{min}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分类树的生成\n",
    "**基尼指数**：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：\n",
    "$$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$$\n",
    "\n",
    "_____\n",
    "对于给定的样本集合$D$，其基尼指数为：$$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$$\n",
    "其中，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。\n",
    "\n",
    "_____\n",
    "如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$，则在特征$A$的条件下，集合$D$的基尼指数为：\n",
    "$$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$\n",
    "$Gini(D)$：表示集合$D$的不确定性；$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。\n",
    "\n",
    "- 算法停止计算的条件是结点中样本的个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
