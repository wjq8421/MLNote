{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN的目标：防止“梯度弥散”。\n",
    "\n",
    "**梯度弥散（梯度消失）**: 通常神经网络所用的激活函数是sigmoid函数，sigmoid函数容易引起梯度弥散。这个函数能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是$f'(x)=f(x)(1-f(x))$，表示两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。\n",
    "\n",
    "**梯度爆炸**：由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。\n",
    "\n",
    "__________\n",
    "梯度消失、梯度爆炸的解决方法：\n",
    "1. 预训练加微调\n",
    "2. 梯度剪切：用于防止梯度爆炸，设置一个梯度剪切值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。\n",
    "3. 权重正则化：用于解决梯度爆炸。\n",
    "4. ReLU、LeakyReLU、elu等激活函数。\n",
    "5. BN（批规范化）：通过规范化操作将输出信号$x$规范化保证网络的稳定性。\n",
    "6. 残差网络。\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN层的优点：\n",
    "1. 加快训练速度，则可使用较大的学习率来训练网络；\n",
    "2. 提高网络的泛化能力；\n",
    "3. BN层本质上是一个归一化层，可以替代局部响应归一化层（LRN层）；\n",
    "4. 可以打乱样本训练顺序。\n",
    "\n",
    "_____\n",
    "Input：Values of $x$ over a mini-batch：$B=\\{x_1,\\cdots, x_m\\}$\n",
    "\n",
    "Output：$\\{y_i=BN_{\\gamma,\\beta}(x_i)\\}$\n",
    "$$\\mu_B=\\frac{1}{m} \\sum_{i=1}m x_i$$\n",
    "$$\\sigma_B^2=\\frac{1}{m}\\sum_{i=1}^m(x_i-\\mu_B)^2$$\n",
    "$$\\hat{x}_i=\\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}$$\n",
    "$$y_i=\\gamma \\hat{x}_i+\\beta \\equiv BN_{\\gamma, \\beta}(x_i)$$\n",
    "\n",
    "在网络训练中以batch-size作为最小单位来不断迭代，每当有新的batch-size进入到网络里面就会产生新的$\\gamma$和$\\beta$。也就是说，我们训练过程中要生成**图片总量/batch_size**组参数。\n",
    "\n",
    "图像卷积的过程中，对于多个卷积核需要保存多个的$\\gamma$和$\\beta$。\n",
    "\n",
    "通常BN网络层用在卷积层后，用于重新调整数据分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "- 归一化数据的目标，是为了让数据的分布变得更加符合期望，增强数据的表达能力。\n",
    "\n",
    "**Internal Covariate Shift**：训练深度网络的时候经常发生训练困难的问题，因为每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。\n",
    "\n",
    "BN之前的解决方案就是使用较小的学习率，和小心的初始化参数，对数据做白化处理，但是显然治标不治本。\n",
    "\n",
    "**Covariate Shift**：主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。\n",
    "______\n",
    "深度学习的本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同（batch梯度下降），那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。\n",
    "\n",
    "深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。\n",
    "\n",
    "_____\n",
    "BN本身也是一种正则的方式，可以代替其他正则方式，如dropout等。\n",
    "\n",
    "BN降低了数据之间的绝对差异，有一个去相关的性质，更多的考虑相对差异性，因此在分类任务上具有更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 参考链接\n",
    "- BN(Batch Normalization) 原理与使用过程详解：https://blog.csdn.net/donkey_1993/article/details/81871132\n",
    "- 关于BN(Batch Normalization)的一些归纳和总结：https://blog.csdn.net/weixin_40533355/article/details/88554586\n",
    "- 梯度弥散与梯度爆炸及其解决方法：https://blog.csdn.net/sinat_41144773/article/details/90712065\n",
    "- 基础 | batchnorm原理及代码详解：https://blog.csdn.net/qq_25737169/article/details/79048516\n",
    "- 深入理解批标准化：https://www.cnblogs.com/guoyaohua/p/8724433.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
