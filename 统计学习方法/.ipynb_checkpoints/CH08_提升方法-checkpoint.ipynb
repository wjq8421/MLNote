{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提升(boosting)**：在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。\n",
    "\n",
    "- Q1：在每一轮如何改变训练数据的权值或概率分布？\n",
    "- Q2：如何将弱分类器组合成一个强分类器？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "#### AdaBoost算法\n",
    "- A1：提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。\n",
    "- A2：采取加权多数表决的方法。具体的，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。\n",
    "\n",
    "_____\n",
    "输入：训练数据集$T=\\{(x_1,y_1), (x_2,y_2),\\dots,(x_N,y_N)\\}$，类标记$y=\\{-1,+1\\}$\n",
    "1. 初始化训练数据的权值分布，假设每个训练样本在基本分类器的学习中作用相同：\n",
    "$$D_1=(w_{11},\\dots,w_{1i},w_{1N}),\\ w_{1i}=\\frac{1}{N},\\ i=1,2\\cdots,N$$\n",
    "2. 对$m=1,2,\\cdots,M$\n",
    "    1. 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x)$\n",
    "    2. 计算$G_m(x)$在训练数据集上的**分类误差率**，其为被$G_m(x)$误分类样本的权值之和：$$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)$$\n",
    "    3. 计算$G_m(x)$的系数，$\\alpha_m$表示$G_m(x)$在最终分类器中的重要性，并且$\\alpha_m$随着$e_m$的减小而增大：$$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$$\n",
    "    4. 更新训练数据集的权值分布，其中，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类的样本权值却得以缩小：$$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$$\n",
    "    $$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i)),\\ i=1,2,\\cdots,N$$\n",
    "    其中，$Z_m$是规范化因子：$$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_my_iG_m(x_i))$$\n",
    "3. 构建基本分类器的线性组合：$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$\n",
    "得到最终分类器：$$G(x)=sign(f(x))=sign(\\sum_{m=1}^M\\alpha_mG_m(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T01:27:09.710408Z",
     "start_time": "2019-04-18T01:27:09.702408Z"
    }
   },
   "source": [
    "__________\n",
    "- AdaBoost最基本的性质：能在学习过程中不断减少训练误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:07:32.375977Z",
     "start_time": "2019-04-18T02:07:32.368977Z"
    }
   },
   "source": [
    "___________\n",
    "#### 提升树(boosting tree)\n",
    "- 以决策树为基函数的提升方法：$$f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m)$$\n",
    "其中，$T(x;\\Theta_m)$表示决策树；$\\Theta_m$为决策树的参数；$M$为树的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T09:14:58.009621Z",
     "start_time": "2019-04-19T09:14:58.002620Z"
    }
   },
   "source": [
    "##### 回归问题的提升树算法\n",
    "1. 求$f_1(x)$，即回归树$T_1(x)$；\n",
    "    1. 通过以下优化问题：$$\\underset{s}{min}[\\underset{c_1}{min}\\sum_{x_i\\in R_i}(y_i-c_i)^2+\\underset{c_2}{min}\\sum_{x_i\\in R_2}(y_i-c_2)^2]$$\n",
    "    2. 求解训练数据的切分点$s$：$$R_1=\\{x|x\\leq s\\},\\ R_2=\\{x|x>s\\}$$\n",
    "    3. 求得在$R_1,\\ R_2$内部使平方误差达到最小值的$c_1,\\ c_2$为：$$c_1=\\frac{1}{N_1}\\sum_{x_i\\in R_1}y_i,\\ c_2=\\frac{1}{N_2}\\sum_{x_i\\in R_2}y_i$$\n",
    "    其中，$N_1,\\ N_2$是$R_1,\\ R_2$的样本点数。此时，回归树$T_1(x)$为：$$T_1(x)=\\left\\{\\begin{matrix}\n",
    "c_1,\\ x\\leq s\\\\ \n",
    "c_2,\\ x> s\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "2. 用$f_1(x)$拟合训练数据的残差：$$r_{2i}=y_i-f_1(x_i),\\ i=1,2,\\cdots,N$$\n",
    "此时，平方损失误差为：$$L(y,f_1(x))=\\sum_{i=1}^N(y_i-f_1(x_i))^2$$\n",
    "\n",
    "3. 与求$T_1(x)$一样，拟合数据的残差，得到$T_2(x)$；\n",
    "此时，$$f_2(x)=T_1(x)+T_2(x)$$\n",
    "4. 不断重复上述步骤，直到平方损失误差满足条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T09:17:39.882879Z",
     "start_time": "2019-04-19T09:17:39.874879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.400000000000002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.38 * 6 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T09:14:53.299351Z",
     "start_time": "2019-04-19T09:14:53.290351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.07 * 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
